{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read rating data\n",
    "fpath = 'BX-Book-Ratings.csv'\n",
    "df = pd.read_csv(fpath, delimiter = \";\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out ISBNs containing 'X'\n",
    "df= df[df.ISBN.apply(lambda x: x.isnumeric())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out any user with less than 20 book ratings\n",
    "valid = df.groupby('User-ID').ISBN.nunique().to_frame()\n",
    "valid = valid.rename(columns={\"ISBN\":\"Freq\"})\n",
    "valid = valid[valid['Freq']>19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting and cut down to 2000 users\n",
    "valid['User-ID'] = valid.index\n",
    "valid = valid.reset_index(drop=True)\n",
    "valid = valid[valid.index<2000]\n",
    "valid = valid.drop('Freq', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with complete dataframe to get ISBNs and individual ratings\n",
    "new_df = pd.merge(df, valid, how= 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix labesl\n",
    "new_df = new_df.rename(columns={\"User-ID\":\"userId\"})\n",
    "new_df= new_df.rename(columns={\"Book-Rating\":\"bookRating\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix type\n",
    "final_df = new_df.astype('int64')\n",
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Sets, Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, holdout_num):\n",
    "    \"\"\" perform training/testing split\n",
    "    \n",
    "    @param df: dataframe\n",
    "    @param holdhout_num: number of items to be held out\n",
    "    \n",
    "    @return df_train: training data\n",
    "    @return df_test testing data\n",
    "    \n",
    "    \"\"\"\n",
    "    # first sort the data by time\n",
    "    #df = df.sort_values(['userId', 'timestamp'], ascending=[True, False])\n",
    "    \n",
    "    # perform deep copy on the dataframe to avoid modification on the original dataframe\n",
    "    df_train = df.copy(deep=True)\n",
    "    df_test = df.copy(deep=True)\n",
    "    \n",
    "    # get test set\n",
    "    #df_test = df_test.groupby(['userId']).head(holdout_num).reset_index()\n",
    "    df_test = df_test.groupby(['userId']).head(holdout_num)\n",
    "    \n",
    "    # get train set\n",
    "    df_train = df_train.merge(\n",
    "        df_test[['userId', 'ISBN']].assign(remove=1),\n",
    "        how='left'\n",
    "    ).query('remove != 1').drop('remove', 1).reset_index(drop=True)\n",
    "    \n",
    "    # sanity check to make sure we're not duplicating/losing data\n",
    "    assert len(df) == len(df_train) + len(df_test)\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we're using 10 because we have a minimum of 20 ratings/user\n",
    "df_train, df_test = train_test_split(new_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(user_ids, book_ids, items, n_neg):\n",
    "    \"\"\"This function creates n_neg negative labels for every positive label\n",
    "    \n",
    "    @param user_ids: list of user ids\n",
    "    @param book_ids: list of movie ids\n",
    "    @param items: unique list of movie ids\n",
    "    @param n_neg: number of negative labels to sample\n",
    "    \n",
    "    @return df_neg: negative sample dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    neg = []\n",
    "    ui_pairs = zip(user_ids, movie_ids)\n",
    "    records = set(ui_pairs)\n",
    "    \n",
    "    # for every positive label case\n",
    "    for (u, i) in records:\n",
    "        # generate n_neg negative labels\n",
    "        for _ in range(n_neg):\n",
    "            # if the randomly sampled movie exists for that user\n",
    "            j = np.random.choice(items)\n",
    "            while(u, j) in records:\n",
    "                # resample\n",
    "                j = np.random.choice(items)\n",
    "            neg.append([u, j, 0])\n",
    "    # conver to pandas dataframe for concatenation later\n",
    "    df_neg = pd.DataFrame(neg, columns=['userId', 'ISBN', 'bookRating'])\n",
    "    \n",
    "    return df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create negative samples for training set\n",
    "neg_train = negative_sampling(\n",
    "    user_ids=df_train.userId.values, \n",
    "    book_ids=df_train.ISBN.values,\n",
    "    items=new_df.ISBN.unique(),\n",
    "    n_neg=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'created {neg_train.shape[0]:,} negative samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative samples = 0, positive samples = 1\n",
    "df_train = df_train[['userId', 'ISBN']].assign(bookRating=1)\n",
    "df_test = df_test[['userId', 'ISBN']].assign(bookRating=1)\n",
    "\n",
    "df_train = pd.concat([df_train, neg_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_count(df):\n",
    "    \"\"\"calculate unique user and movie counts\"\"\"\n",
    "    return df.userId.nunique(), df.ISBN.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training set shape', get_unique_count(df_train))\n",
    "print('testing set shape', get_unique_count(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique user and number of unique item/movie\n",
    "n_user, n_item = get_unique_count(df_train)\n",
    "\n",
    "print(\"number of unique users\", n_user)\n",
    "print(\"number of unique items\", n_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the variable for the model training notebook\n",
    "# -----\n",
    "%store n_user\n",
    "%store n_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current session region\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'currently in {region}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the default sagemaker s3 bucket to store processed data\n",
    "# here we figure out what that default bucket name is \n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "print(bucket_name)  # bucket name format: \"sagemaker-{region}-{aws_account_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data locally first\n",
    "dest = 'data/s3'\n",
    "train_path = os.path.join(dest, 'train.npy')\n",
    "test_path = os.path.join(dest, 'test.npy')\n",
    "\n",
    "!mkdir {dest}\n",
    "np.save(train_path, df_train.values, allow_pickle =False)\n",
    "np.save(test_path, df_test.values, allow_pickle =False)\n",
    "\n",
    "# upload to S3 bucket (see the bucket name above)\n",
    "sagemaker_session.upload_data(train_path, key_prefix='data')\n",
    "sagemaker_session.upload_data(test_path, key_prefix='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
